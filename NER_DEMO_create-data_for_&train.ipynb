{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad21876",
   "metadata": {},
   "source": [
    "# NER\n",
    "В этом документе я создаю теги для нужных нам сущностей, обучаю модель распознавания на них и вывожу результат этого распознавания\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3746b",
   "metadata": {},
   "source": [
    "В этой ячейке проверяем, как работает модель spaCy для русского языка. Поскольку сейчас исопльзуется предобученная на датасете новостей модель, она умеет определять только формальные сущности (персона, локация, время), которые неприменимы к нашему проекту. Поэтому далее я буду создавать распознаватель конкретно под наши задачи. \n",
    "\n",
    "А в этой ячейке удостоверяемся, что spaCy корректно отвечает на заданные для нее условия (определила \"Ульяна\" как \"персону\", \"Москве\" как \"локацию\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced7fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from spacy.lang.ru import Russian\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "import ru_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16a7b5",
   "metadata": {},
   "source": [
    "Считываем датасет всех команд, далее выбираем отдельно столбец с командами для дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaca18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_csv('all_commands.csv')\n",
    "all_commands = pd.read_csv('all_commands.csv')[['command']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa8db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>command</th>\n",
       "      <th>intent</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>я быть в отчаяние чтобы пойти направо</td>\n",
       "      <td>move_ship_by_direction</td>\n",
       "      <td>ship_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>я пойти наверх корабль на</td>\n",
       "      <td>move_ship_by_direction</td>\n",
       "      <td>ship_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>давать подняться наверх</td>\n",
       "      <td>move_ship_by_direction</td>\n",
       "      <td>ship_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>пожалуйста слева слышать</td>\n",
       "      <td>move_ship_by_direction</td>\n",
       "      <td>ship_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>пойти я на корабль</td>\n",
       "      <td>move_ship_by_direction</td>\n",
       "      <td>ship_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11109</th>\n",
       "      <td>11109</td>\n",
       "      <td>возвращаться к свой пиратка</td>\n",
       "      <td>pirate_rebirth</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11110</th>\n",
       "      <td>11110</td>\n",
       "      <td>я хотеть крепость в отдохнуть</td>\n",
       "      <td>pirate_rebirth</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>11111</td>\n",
       "      <td>я хотеть отдохнуть в крепость</td>\n",
       "      <td>pirate_rebirth</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11112</th>\n",
       "      <td>11112</td>\n",
       "      <td>крепость хотеть отдохнуть в я</td>\n",
       "      <td>pirate_rebirth</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>11113</td>\n",
       "      <td>в хотеть отдохнуть я крепость</td>\n",
       "      <td>pirate_rebirth</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11114 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                command  \\\n",
       "0               0  я быть в отчаяние чтобы пойти направо   \n",
       "1               1              я пойти наверх корабль на   \n",
       "2               2                давать подняться наверх   \n",
       "3               3               пожалуйста слева слышать   \n",
       "4               4                     пойти я на корабль   \n",
       "...           ...                                    ...   \n",
       "11109       11109            возвращаться к свой пиратка   \n",
       "11110       11110          я хотеть крепость в отдохнуть   \n",
       "11111       11111          я хотеть отдохнуть в крепость   \n",
       "11112       11112          крепость хотеть отдохнуть в я   \n",
       "11113       11113          в хотеть отдохнуть я крепость   \n",
       "\n",
       "                       intent          entity  \n",
       "0      move_ship_by_direction  ship_direction  \n",
       "1      move_ship_by_direction  ship_direction  \n",
       "2      move_ship_by_direction  ship_direction  \n",
       "3      move_ship_by_direction  ship_direction  \n",
       "4      move_ship_by_direction  ship_direction  \n",
       "...                       ...             ...  \n",
       "11109          pirate_rebirth            none  \n",
       "11110          pirate_rebirth            none  \n",
       "11111          pirate_rebirth            none  \n",
       "11112          pirate_rebirth            none  \n",
       "11113          pirate_rebirth            none  \n",
       "\n",
       "[11114 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeea1dd",
   "metadata": {},
   "source": [
    "Переводим колонку команд в список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08626be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_commands_list = []\n",
    "for i in all_commands['command']:\n",
    "        all_commands_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b3e59",
   "metadata": {},
   "source": [
    "Функция для записи списка команд в JSON файл, с ним далее удобнее работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c677de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(a_list):\n",
    "    with open(\"all_commands.json\", \"w\", encoding='utf8') as f:\n",
    "        json.dump(a_list, f, ensure_ascii=False)\n",
    "\n",
    "write_list(all_commands_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1bedc",
   "metadata": {},
   "source": [
    "Заводим 2 функции для чтения файла и сохранения в файл. Загружаем из JSON, сохраняем тоже в JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e7ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "\n",
    "def save_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528c8ce",
   "metadata": {},
   "source": [
    "Пишем функции для обработки команд. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da5e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В отдельном файле у нас есть список сущностей и их лэйбл. Библиотека spaCy требует особой \n",
    "# структуры данных для обработки пар сущность — лэйбл (это паттерн). Эта функция позволяет создать такую структуру\n",
    "# из имеющихся данных \n",
    "\n",
    "def create_training_data(file, type):\n",
    "    data = load_data(file)\n",
    "    patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\n",
    "                    \"label\": type,\n",
    "                    \"pattern\": item\n",
    "                    }\n",
    "        patterns.append(pattern)\n",
    "    return (patterns)\n",
    "\n",
    "\n",
    "\n",
    "# Эта фунция создает и сохраняет кастомную модель NER, которая работает с созданными выше паттернами.\n",
    "\n",
    "def generate_rules(patterns):\n",
    "    nlp = Russian()\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.to_disk(\"jackal_ner_all_entities\")\n",
    "    \n",
    "\n",
    "    \n",
    "# Эта функция обрабатывает входящий текст (ищет сущности) с использованием созданной выше модели и записывает\n",
    "# найденное в список \n",
    "    \n",
    "def test_model(model, text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\": entities}] # специальный формат для spaCy\n",
    "        return (results)\n",
    "                \n",
    "\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953f3d5",
   "metadata": {},
   "source": [
    "Создаем паттерны для каждого типа сущностей и объединяем в единый список — это нужно, чтобы конечная модель имела в себе паттерны сущностей всех типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d32f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_dir = create_training_data(\"NER_dir.json\", \"DIR\")\n",
    "patterns_tile = create_training_data(\"NER_tiles.json\", \"TILE\")\n",
    "pattern_act = create_training_data(\"NER_act.json\", \"ACT\")\n",
    "pattern_num = create_training_data(\"NER_num.json\", \"NUM\")\n",
    "\n",
    "all_patterns = patterns_dir + patterns_tile + pattern_act + pattern_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3953e30",
   "metadata": {},
   "source": [
    "Создаем модель распознавания сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d0cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rules(all_patterns)\n",
    "\n",
    "# Объединила паттерны и сделала единую модель, чтобы иметь множество лейблов, а не один\n",
    "# print (patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed7328",
   "metadata": {},
   "source": [
    "Функция ниже создает список команд из JSON файла, а далее создает тренировочный размеченный датасет в 70% от всего объема команд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "123ab334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file):\n",
    "    data = load_data(file)\n",
    "    text = []\n",
    "    for item in data:\n",
    "        text.append(item) \n",
    "    return (text)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"jackal_ner_all_entities\")\n",
    "ALL_DATA = []\n",
    "outsiders = []\n",
    "text = get_text(\"all_commands.json\")\n",
    "hits = []\n",
    "counter = 0\n",
    "test_size = round(0.7 * len(text))\n",
    "while counter < len(text): # делаем тренировочный датасет в 70% от всего\n",
    "    for command in text:\n",
    "        command = command.strip()\n",
    "    #        command = command.replace(\"\\n\", \" \")\n",
    "        results = test_model(nlp, command)\n",
    "        if results != None:\n",
    "            ALL_DATA.append(results)\n",
    "            \n",
    "        ########################\n",
    "        #далее идут временный команды. нужны для отслеживания качества\n",
    "        else:\n",
    "             outsiders.append(command)\n",
    "            \n",
    "        \n",
    "        #######################\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ba48a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = ALL_DATA[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c38bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ALL_DATA[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2a75cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"ML_NER_train_data.json\", TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8261523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ALL_DATA = pd.DataFrame(ALL_DATA) # стандартный брейк на трейн и тест не производился, поскольку далее нужны json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dcaecab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ALL_DATA[[0]]\n",
    "y = ALL_DATA[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a203f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9aaa4",
   "metadata": {},
   "source": [
    "Сохраняем тренировочный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd9d9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "TRAIN_DATA.to_pickle(\"ML_NER_train_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69a959",
   "metadata": {},
   "source": [
    "## Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "8a556c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "def train_spacy(data, epochs):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank(\"ru\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.select_pipes(disable=other_pipes):\n",
    "        optimizer = nlp.initialize()\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    sgd=optimizer,\n",
    "                    losses=losses\n",
    "                )\n",
    "            print(f\"Epoch {epoch}, losses {losses}\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "9bb8eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = load_data(\"ML_NER_train_data.json\")\n",
    "random.shuffle(TRAIN_DATA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4baa1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, losses {'ner': 685.8730080071026}\n",
      "Epoch 1, losses {'ner': 183.33603484448463}\n",
      "Epoch 2, losses {'ner': 100.24093842858794}\n",
      "Epoch 3, losses {'ner': 129.72534167452804}\n",
      "Epoch 4, losses {'ner': 65.5520702693125}\n",
      "Epoch 5, losses {'ner': 77.73507472944524}\n",
      "Epoch 6, losses {'ner': 80.61540234219471}\n",
      "Epoch 7, losses {'ner': 53.952361240802205}\n",
      "Epoch 8, losses {'ner': 18.3246503182347}\n",
      "Epoch 9, losses {'ner': 72.16701022966815}\n",
      "Epoch 10, losses {'ner': 51.49825774152919}\n",
      "Epoch 11, losses {'ner': 45.28911730945829}\n",
      "Epoch 12, losses {'ner': 2.1879180613981437}\n",
      "Epoch 13, losses {'ner': 45.47298765742108}\n",
      "Epoch 14, losses {'ner': 32.44536760498236}\n",
      "Epoch 15, losses {'ner': 35.90443976631265}\n",
      "Epoch 16, losses {'ner': 6.027575720772911e-06}\n",
      "Epoch 17, losses {'ner': 21.20078511158035}\n",
      "Epoch 18, losses {'ner': 26.756651870979084}\n",
      "Epoch 19, losses {'ner': 27.73650418823898}\n",
      "Epoch 20, losses {'ner': 74.25410757470138}\n",
      "Epoch 21, losses {'ner': 18.282319861887448}\n",
      "Epoch 22, losses {'ner': 18.1312045236756}\n",
      "Epoch 23, losses {'ner': 36.8232212681456}\n",
      "Epoch 24, losses {'ner': 36.293488382942016}\n",
      "Epoch 25, losses {'ner': 3.307479274369041e-05}\n",
      "Epoch 26, losses {'ner': 12.316497538361382}\n",
      "Epoch 27, losses {'ner': 21.645476957896307}\n",
      "Epoch 28, losses {'ner': 40.68329795302687}\n",
      "Epoch 29, losses {'ner': 35.883532718464714}\n"
     ]
    }
   ],
   "source": [
    "nlp = train_spacy(TRAIN_DATA, 30)\n",
    "nlp.to_disk(\"jackal_ner_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a527107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pabakst/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "9b67963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def input_for_gui (test):\n",
    "    arrow = ['стрелка', 'указатель']\n",
    "    balloon = ['воздушный шар', 'шар']\n",
    "    barrel = ['бочка']\n",
    "    cannibal = ['людоед']\n",
    "    castle = ['крепость']\n",
    "    castle_girl = [\"девушка\", \"абориген\", \"аборигенка\"]\n",
    "    croc = ['крокодил']\n",
    "    gold = ['сундук', 'деньги', 'сокровища',\"золотишко\", \"монета\", \"золото\", \"деньга\", \"монетка\", \n",
    "            \"клад\", \"сундучок\", \"мелочь\", \"сокровище\", \"сокровищница\"]\n",
    "    horse = ['конь', 'лошадь']\n",
    "    ice = ['лед', 'лёд']\n",
    "    labyrinth = [ 'лабиринт', 'джунгли', 'пустыня', 'болото',\n",
    "        'горы', \"тропик\", \"пустынь\", \"заросль\", \"гора\", \"скала\" , \"лес\", \n",
    "         \"песок\", \"дюна\",\"тропический\", \"леса\"]\n",
    "    plane = ['самолет', \"самолёт\"]\n",
    "    trap = ['капкан', 'ловушка']\n",
    "    cannon = ['пушка']\n",
    "    field = ['поляна', 'пустышка', \"клетка\",\n",
    "         \"клеточка\",\"холм\"]\n",
    "    \n",
    "    \n",
    "#     test = 'первым пиратом пойду налево и попаду на шар'\n",
    "    test = preprocess_text(test)\n",
    "    nlp = spacy.load(\"jackal_ner_trained_model\") # сущности определяются посредством обучения модели\n",
    "    doc = nlp(test)\n",
    "    results = test_model(nlp, command)\n",
    "    dict_ = {}\n",
    "    for ent in doc.ents:\n",
    "        dict_[ent.label_] = ent.text\n",
    "#         print (ent.text, ent.label_)\n",
    "    print(dict_)\n",
    "    \n",
    "    if \"NUM\" in dict_.keys():\n",
    "        if \"пер\" in dict_[\"NUM\"]:\n",
    "            number = 1\n",
    "        elif \"втор\" in dict_[\"NUM\"]:\n",
    "            number = 2     \n",
    "        elif \"тр\" in dict_[\"NUM\"]:\n",
    "            number = 3\n",
    "    else:\n",
    "        number = 101\n",
    "        \n",
    "    if \"DIR\" in dict_.keys():\n",
    "        if \"лев\" in dict_[\"DIR\"]:\n",
    "            direction = 0\n",
    "        elif \"прав\" in dict_[\"DIR\"]:\n",
    "            direction = 1   \n",
    "        elif \"прям\" in dict_[\"DIR\"]:\n",
    "            direction = 2  \n",
    "        elif \"наз\" in dict_[\"DIR\"]:\n",
    "            direction = 3\n",
    "    \n",
    "        return number, direction   \n",
    "            \n",
    "    elif \"DIR\" not in dict_.keys()and \"TILE\" in dict_.keys() :\n",
    "        if dict_[\"TILE\"] in arrow:\n",
    "            tile = 1\n",
    "        elif dict_[\"TILE\"] in balloon:\n",
    "            tile = 8   \n",
    "        elif dict_[\"TILE\"] in barrel:\n",
    "            tile = 9     \n",
    "        elif dict_[\"TILE\"] in cannibal:\n",
    "            tile = 10   \n",
    "        elif dict_[\"TILE\"] in castle:\n",
    "            tile = 11             \n",
    "        elif dict_[\"TILE\"] in castle_girl:\n",
    "            tile = 12  \n",
    "        elif dict_[\"TILE\"] in croc:\n",
    "            tile = 13 \n",
    "        elif dict_[\"TILE\"] in gold:\n",
    "            tile = 14\n",
    "        elif dict_[\"TILE\"] in horse:\n",
    "            tile = 19            \n",
    "        elif dict_[\"TILE\"] in ice:\n",
    "            tile = 20            \n",
    "        elif dict_[\"TILE\"] in labyrinth:\n",
    "            tile = 21   \n",
    "        elif dict_[\"TILE\"] in field:\n",
    "            tile = 25               \n",
    "        elif dict_[\"TILE\"] in plane:\n",
    "            tile = 29               \n",
    "        elif dict_[\"TILE\"] in trap:\n",
    "            tile = 30   \n",
    "        elif dict_[\"TILE\"] in cannon:\n",
    "            tile = 31             \n",
    "    \n",
    "        return number, tile  \n",
    "            \n",
    "# print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a6134cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACT': 'пират', 'TILE': 'шар'}\n"
     ]
    }
   ],
   "source": [
    "input_for_gui(\"первым пиратом пойду попаду на шар\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b297a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = 'первым пиратом пойду налево и попаду на шар'\n",
    "\n",
    "\n",
    "# test = preprocess_text(test)\n",
    "# nlp = spacy.load(\"jackal_ner_all_entities\") # все сущности посчитаны искусственно, механически\n",
    "# doc = nlp(test)\n",
    "# results = test_model(nlp, command)\n",
    "# for ent in doc.ents:\n",
    "#     print (ent.text, ent.label_)\n",
    "# # print(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90d768",
   "metadata": {},
   "source": [
    "## Считаем f-меру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5b8baf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "104c2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"jackal_ner_trained_model\")\n",
    "y_pred = []\n",
    "for i in X_test_list:\n",
    "#     test = preprocess_text(i) -- меняет индекс вхождения слова, осторожно при сравнении по индексу вхождения\n",
    "    results = test_model(nlp, i)\n",
    "    if results is None:\n",
    "        y_pred.append(\"NaN\")\n",
    "#         print('NaN')\n",
    "    else:\n",
    "        y_pred.append(results[1])\n",
    "#         print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "7eacb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test[1].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "a66f65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# надо сравнить значения на y_pred и y_true, отношение y_pred к y_test\n",
    "\n",
    "# print(f'type(y_true) = {type(y_true)}, type(y_pred) = {type(y_pred)}')\n",
    "# len(y_pred) == len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873dae6",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ad177724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN {'entities': [(2, 7, 'TILE')]}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[12], y_true[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "30acdd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отношение верных ответов ко всему кол-ву строк (сравнение по индексам): 0.9741379310344828\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "\n",
    "for i in range(0,len(y_true)):\n",
    "    if y_true[i] == y_pred[i]:\n",
    "        TP += 1\n",
    "    elif y_true[i] != \"NaN\" and y_pred[i] == \"NaN\":\n",
    "        FN += 1\n",
    "    elif y_true[i] == \"NaN\" and y_pred[i] != \"NaN\":\n",
    "        FP += 1     \n",
    "    elif y_true[i] == \"NaN\" and y_pred[i] == \"NaN\":\n",
    "        TN += 1 \n",
    "        \n",
    "print(f'Отношение верных ответов ко всему кол-ву строк (сравнение по индексам): {TP/len(y_true)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "79b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "bd7e244b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "значения для NER модели равна: (2938, 0, 0, 46)\n"
     ]
    }
   ],
   "source": [
    "print(f'значения для NER модели равна: {TP, TN, FP, FN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a1193a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ручная f-мера\n",
    "\n",
    "def f1_score (TP, TN, FP, FN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "91c78658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9922323539344816\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(TP, TN, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "be9afd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера NER модели равна: 0.9922323539344816\n"
     ]
    }
   ],
   "source": [
    "print(f'F1-мера NER модели равна: {f1_score(TP, TN, FP, FN)}') # но на самом деле так потому что нет TN и FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0719ee",
   "metadata": {},
   "source": [
    "## Эра другой разметки для подсчета более значимых метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca876a9",
   "metadata": {},
   "source": [
    "### Рассмотрим то же самое на другом формате — не в индексах, а в слово + класс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e2044",
   "metadata": {},
   "source": [
    "#### Исследуем возможность fine-tune'a модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd23c1",
   "metadata": {},
   "source": [
    "1) Источник: https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b653e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем сырой датасет с колонками: слово — метка\n",
    "# # написать цикл:\n",
    "#     # для каждой команды в модели:\n",
    "#     токенизировать, собрать в список\n",
    "    \n",
    "#     проходимся по списку и обрабатываем каждое слово: колонка со словом — колонка с меткой, \n",
    "#         метку ставить на слово, если оно есть в списке из подгруженных файлов (см как подгружала паттерны)\n",
    "    \n",
    "#     # \n",
    "#     вывести датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5970653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m455.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.0-cp39-cp39-macosx_10_14_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m898.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.4)\n",
      "Requirement already satisfied: packaging in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting responses<0.19 (from datasets)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m795.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 pyarrow-12.0.0 responses-0.18.0 xxhash-3.2.0\n",
      "Requirement already satisfied: tokenizers in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (0.11.4)\n",
      "Requirement already satisfied: transformers in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (4.18.0)\n",
      "Requirement already satisfied: filelock in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: six in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5f6df5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466034d6b74a4de3a2182db60a9c2bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabce592138e4eb4a4b4334c5cc29422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/617k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7120c2e57985453aac3b53cc6d02946e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikiann/bn to /Users/pabakst/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733b3a21b67f4d0ebcb63744914b561d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikiann downloaded and prepared to /Users/pabakst/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acb990c03824b38add54031a4d86281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8c5a03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "93b08e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4ee73f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_our = ['O', 'B-ACT', 'B-NUM', 'B-DIR', 'B-TILE']\n",
    "# label_names_our = ['O', 'B-ACT', 'I-ACT', 'B-NUM', 'I-NUM', 'B-DIR', 'I-DIR', 'B-TILE', 'I-TILE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4ce7ca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-ACT', 'B-NUM', 'B-DIR', 'B-TILE']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names_our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# продолжить из статей..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "96496d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем один длинный текст из всего датасета:\n",
    "all_commands_str = ' '.join(all_commands_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f056d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = nlp(all_commands_str)\n",
    "\n",
    "all_tokens = [token.text for token in all_doc]\n",
    "# print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "af0aa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DIR = load_data(\"NER_dir.json\")\n",
    "data_TILE = load_data(\"NER_tile.json\")\n",
    "data_ACT = load_data(\"NER_act.json\")\n",
    "data_NUM = load_data(\"NER_NUM.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "7158e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "word = []\n",
    "label = []\n",
    "# \n",
    "for i in range(len(all_tokens)):\n",
    "    current_word = all_tokens[i]\n",
    "    word.append(current_word)\n",
    "    \n",
    "    if current_word in data_DIR:\n",
    "#         if i != 1 and all_tokens[i-1] in data_DIR:\n",
    "#             label.append('I-DIR')\n",
    "#         else:\n",
    "            label.append('B-DIR')\n",
    "            \n",
    "    elif current_word in data_ACT:\n",
    "#         if i != 1 and all_tokens[i-1] in data_ACT:\n",
    "#             label.append('I-ACT')\n",
    "#         else:\n",
    "            label.append('B-ACT')\n",
    "            \n",
    "    elif current_word in data_NUM:\n",
    "#         if i != 1 and all_tokens[i-1] in data_NUM:\n",
    "#             label.append('I-NUM')\n",
    "#         else:\n",
    "            label.append('B-NUM')\n",
    "\n",
    "    elif current_word in data_TILE:\n",
    "#         if i != 1 and all_tokens[i-1] in data_TILE:\n",
    "#             label.append('I-TILE')\n",
    "#         else:\n",
    "            label.append('B-TILE')\n",
    "\n",
    "    else:\n",
    "        label.append('O')\n",
    "        \n",
    "        \n",
    "d[\"word\"] = word\n",
    "d[\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "3f2b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_labeled_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "fcbf4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_labeled_df.to_csv('labeled.tsv',\n",
    "sep='\\t', # Tab separator\n",
    "header=True,\n",
    "index=False,\n",
    "index_label = False,\n",
    "encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eec34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "22e5987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .tsv file to json format. \n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Файл не конвертируется\" + \"\\n\" + \"ошибка = \" + str(e))\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "ae029768",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_to_json_format(\"labeled.tsv\",'labeled.json','abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb52f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ab0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f4bd9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_labeled_df_2 = words_labeled_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458dacc",
   "metadata": {},
   "source": [
    "# японский туториал:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1985b0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2533609562.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [302]\u001b[0;36m\u001b[0m\n\u001b[0;31m    sudo apt-get update\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# # compilers and development settings\n",
    "# sudo apt-get update\n",
    "# sudo apt install -y gcc\n",
    "# sudo apt-get install -y make\n",
    "\n",
    "# # install CUDA 11.4.4 (because I use old generation K80 GPU)\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux.run\n",
    "# sudo sh cuda_11.4.4_470.82.01_linux.run\n",
    "# echo -e \"export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64\" >> ~/.bashrc\n",
    "# source ~/.bashrc\n",
    "\n",
    "# # install and upgrade pip\n",
    "# sudo apt-get install -y python3-pip\n",
    "# sudo -H pip3 install --upgrade pip\n",
    "\n",
    "# # install pytorch with GPU accelerated\n",
    "# # (see https://pytorch.org/get-started/locally/ )\n",
    "# pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu114\n",
    "\n",
    "# # install sentencepiece for multi-lingual modeling\n",
    "# pip3 install omegaconf hydra-core fairseq sentencepiece\n",
    "\n",
    "# # install huggingface transformer with deepspeed\n",
    "# sudo apt install python3-mpi4py\n",
    "# sudo apt-get install ninja-build\n",
    "# pip3 install transformers[deepspeed] datasets\n",
    "\n",
    "# # install additional packages\n",
    "# pip3 install numpy seqeval pandas matplotlib scikit-learn\n",
    "\n",
    "# # install jupyter if you run code in notebook\n",
    "# pip3 install jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "43bc3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "# load dataset\n",
    "with open(\"ner_yap.json\") as f:\n",
    "  json_all = json.load(f)\n",
    "\n",
    "# create chracater-based annotated dataset\n",
    "tokens_list = []\n",
    "ner_tags_list = []\n",
    "for json_dat in json_all:\n",
    "  tokens = list(json_dat[\"text\"])\n",
    "  ner_tags = [\"O\"] * len(tokens)\n",
    "  for ent in json_dat[\"entities\"]:\n",
    "    for i in range(ent[\"span\"][0], ent[\"span\"][1]):\n",
    "      # See https://github.com/stockmarkteam/ner-wikipedia-dataset\n",
    "      if ent[\"type\"] == \"人名\":  # person\n",
    "        ner_tags[i] = \"PER\"\n",
    "      elif ent[\"type\"] == \"法人名\":  # organization (corporation general)\n",
    "        ner_tags[i] = \"ORG\"\n",
    "      elif ent[\"type\"] == \"政治的組織名\":  # organization (political)\n",
    "        ner_tags[i] = \"ORG-P\"\n",
    "      elif ent[\"type\"] == \"その他の組織名\":  # organization (others)\n",
    "        ner_tags[i] = \"ORG-O\"\n",
    "      elif ent[\"type\"] == \"地名\":  # location\n",
    "        ner_tags[i] = \"LOC\"\n",
    "      elif ent[\"type\"] == \"施設名\":  # institution (facility)\n",
    "        ner_tags[i] = \"INS\"\n",
    "      elif ent[\"type\"] == \"製品名\":  # product\n",
    "        ner_tags[i] = \"PRD\"\n",
    "      elif ent[\"type\"] == \"イベント名\":  # event\n",
    "        ner_tags[i] = \"EVT\"\n",
    "  tokens_list.append(tokens)\n",
    "  ner_tags_list.append(ner_tags)\n",
    "\n",
    "features = Features({\n",
    "  \"tokens\": Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "  \"ner_tags\": Sequence(feature=ClassLabel(names=[\"O\", \"PER\", \"ORG\", \"ORG-P\", \"ORG-O\", \"LOC\", \"INS\", \"PRD\", \"EVT\"], id=None), length=-1, id=None)\n",
    "})\n",
    "ds = Dataset.from_dict(\n",
    "  {\"tokens\": tokens_list, \"ner_tags\": ner_tags_list},\n",
    "  features=features\n",
    ")\n",
    "\n",
    "# generate converter for index(int)-to-tag(string) and tag(string)-to-index(int)\n",
    "tags = ds.features[\"ner_tags\"].feature\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "# separate dataset into train dataset and validation dataset\n",
    "ds = ds.train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "3dfb6358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'PER', 'ORG', 'ORG-P', 'ORG-O', 'LOC', 'INS', 'PRD', 'EVT'], id=None)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "ff66b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "\n",
    "features = Features({\n",
    "  \"tokens\": Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "  \"ner_tags\": Sequence(feature=ClassLabel(names=['O', 'ACT', 'NUM', 'DIR', 'TILE'], id=None), length=-1, id=None)\n",
    "})\n",
    "# ds = Dataset.from_dict(\n",
    "#   {\"tokens\": word, \"ner_tags\": label},\n",
    "#   features=features\n",
    "# )\n",
    "\n",
    "# generate converter for index(int)-to-tag(string) and tag(string)-to-index(int)\n",
    "tags = features[\"ner_tags\"].feature\n",
    "# index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "# tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "# # separate dataset into train dataset and validation dataset\n",
    "# ds = ds.train_test_split(test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790f91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "23d9d440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/pabakst/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /Users/pabakst/.cache/huggingface/transformers/tmpe2ujnj88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b232834e1c41d988fa9d1929a42c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model in cache at /Users/pabakst/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "creating metadata file for /Users/pabakst/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /Users/pabakst/.cache/huggingface/transformers/tmpxjwnekv2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2e2066a4e46dfa6c34cc8dc26ab5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json in cache at /Users/pabakst/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "creating metadata file for /Users/pabakst/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /Users/pabakst/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /Users/pabakst/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/pabakst/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokenizer of pre-trained XML-RoBERTa model\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# define function for dataset conversion\n",
    "def tokenize_and_align_labels(data):\n",
    "  text = [\"\".join(t) for t in data[\"tokens\"]]\n",
    "  # tokenized_inputs = xlmr_tokenizer(text, truncation=True, max_length=512)\n",
    "  tokenized_inputs = xlmr_tokenizer(text)\n",
    "\n",
    "  #\n",
    "  # map label to the new token\n",
    "  #\n",
    "  # [example]\n",
    "  #   org token (data)      : [\"松\", \"崎\", \"は\", \"日\", \"本\", \"に\", \"い\", \"る\"]\n",
    "  #   new token (tokenized_inputs): [\"_\", \"松\", \"崎\", \"は\", \"日本\", \"に\", \"いる\"]\n",
    "  labels = []\n",
    "  for row_idx, label_old in enumerate(data[\"ner_tags\"]):\n",
    "    # label is initialized as [[], [], [], [], [], [], []]\n",
    "    label_new = [[] for t in tokenized_inputs.tokens(batch_index=row_idx)]\n",
    "    # label becomes [[1], [1], [1], [0], [5, 5], [0], [0, 0]]\n",
    "    for char_idx in range(len(data[\"tokens\"][row_idx])):\n",
    "      token_idx = tokenized_inputs.char_to_token(row_idx, char_idx)\n",
    "      if token_idx is not None:\n",
    "        label_new[token_idx].append(data[\"ner_tags\"][row_idx][char_idx])\n",
    "        if (tokenized_inputs.tokens(batch_index=row_idx)[token_idx] == \"▁\") and (data[\"ner_tags\"][row_idx][char_idx] != 0):\n",
    "          label_new[token_idx+1].append(data[\"ner_tags\"][row_idx][char_idx])\n",
    "    # label becomes [1, 1, 1, 0, 5, 0, 0]\n",
    "    label_new = list(map(lambda i : max(i, default=0), label_new))\n",
    "    # append result\n",
    "    labels.append(label_new)\n",
    "\n",
    "  tokenized_inputs[\"labels\"] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "# run conversion\n",
    "tokenized_ds = ds.map(\n",
    "  tokenize_and_align_labels,\n",
    "  remove_columns=[\"tokens\", \"ner_tags\"],\n",
    "  batched=True,\n",
    "  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1d583129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/pabakst/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"ACT\",\n",
      "    \"2\": \"NUM\",\n",
      "    \"3\": \"DIR\",\n",
      "    \"4\": \"TILE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ACT\": 1,\n",
      "    \"DIR\": 3,\n",
      "    \"NUM\": 2,\n",
      "    \"O\": 0,\n",
      "    \"TILE\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/pabakst/.cache/huggingface/transformers/tmpdo138omv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1322d80b53a4dbaa836c7fe0a3bb635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin in cache at /Users/pabakst/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "creating metadata file for /Users/pabakst/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /Users/pabakst/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaForTokenClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(\n",
    "  \"xlm-roberta-base\",\n",
    "  num_labels=tags.num_classes,\n",
    "  id2label=index2tag,\n",
    "  label2id=tag2index\n",
    ")\n",
    "model = (RobertaForTokenClassification\n",
    "         .from_pretrained(\"xlm-roberta-base\", config=xlmr_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ec8b669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class MyCustomizationSampleModel(RobertaPreTrainedModel):\n",
    "  _keys_to_ignore_on_load_unexpected = [r\"pooler\"]  # because we don't add pooling\n",
    "  _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    \n",
    "    #\n",
    "    # The name of layer (\"roberta\", etc) is very important !\n",
    "    # When you change these names, these weights and bias might be ignored in saving checkpoint.\n",
    "    #\n",
    "\n",
    "    self.num_labels = config.num_labels\n",
    "    # hf roberta model\n",
    "    self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "    # linear for classification\n",
    "    self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.linear = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "    # initialize weights\n",
    "    ### self.init_weights()\n",
    "    self.post_init()\n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "    # build model\n",
    "    roberta_output = self.roberta(\n",
    "      input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids, # this will always be None\n",
    "      **kwargs\n",
    "    )\n",
    "    x = self.dropout(roberta_output[0])\n",
    "    logits = self.linear(x)\n",
    "    # calculate loss if labels are provided\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "      loss = cross_entropy(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    # return result\n",
    "    return TokenClassifierOutput(\n",
    "      loss=loss,\n",
    "      logits=logits,\n",
    "      hidden_states=roberta_output.hidden_states,\n",
    "      attentions=roberta_output.attentions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "7e9a4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir = \"xlm-roberta-ner-try\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 1,\n",
    "  per_device_train_batch_size = 12,\n",
    "  per_device_eval_batch_size = 12,\n",
    "  evaluation_strategy = \"epoch\",\n",
    "#   fp16 = True,\n",
    "  logging_steps = len(tokenized_ds[\"train\"]),\n",
    "  push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "725b6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "  xlmr_tokenizer,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "bb287a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "  preds = np.argmax(eval_arg.predictions, axis=2)\n",
    "  batch_size, seq_len = preds.shape\n",
    "  y_true, y_pred = [], []\n",
    "  for b in range(batch_size):\n",
    "    true_label, pred_label = [], []\n",
    "    for s in range(seq_len):\n",
    "      if eval_arg.label_ids[b, s] != -100:  # -100 must be ignored\n",
    "        true_label.append(index2tag[eval_arg.label_ids[b][s]])\n",
    "        pred_label.append(index2tag[preds[b][s]])\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "  return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4dc69d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  compute_metrics = metrics_func,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"test\"],\n",
    "  tokenizer = xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "46d19d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4808\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2107\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: ERROR Error while calling W&B API: user is not logged in (<Response [401]>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c45840552c4b9b913f822bf5465c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675096110011509, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py 593 setup\n"
     ]
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 60.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [419]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1354\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m   1352\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;66;03m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mignore_data_skip:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py:347\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    346\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py:388\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 388\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py:621\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py:593\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrun_name\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1171\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1170\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m-> 1171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1152\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1150\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:768\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 60.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "0cfa2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# save fine-tuned model in local\n",
    "os.makedirs(\"./trained_ner_classifier_try\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "  trainer.model.module.save_pretrained(\"./trained_ner_classifier_try\")\n",
    "else:\n",
    "  trainer.model.save_pretrained(\"./trained_ner_classifier_try\")\n",
    "\n",
    "# load from the saved checkpoint\n",
    "xlmr_config = AutoConfig.from_pretrained(\n",
    "  \"xlm-roberta-base\",\n",
    "  num_labels=tags.num_classes,\n",
    "  id2label=index2tag,\n",
    "  label2id=tag2index\n",
    ")\n",
    "model = (RobertaForTokenClassification\n",
    "         .from_pretrained(\"./trained_ner_classifier_try\", config=xlmr_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "46476ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁первым</td>\n",
       "      <td>▁</td>\n",
       "      <td>пира</td>\n",
       "      <td>том</td>\n",
       "      <td>▁пойду</td>\n",
       "      <td>▁на</td>\n",
       "      <td>ле</td>\n",
       "      <td>во</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1  2     3    4       5    6   7   8     9\n",
       "Tokens  <s>  ▁первым  ▁  пира  том  ▁пойду  ▁на  ле  во  </s>\n",
       "Tags      O        O  O     O    O       O    O   O   O     O"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# create dataset for prediction\n",
    "sample_encoding = xlmr_tokenizer([\n",
    "  \"первым пиратом пойду налево\",\n",
    "], truncation=True, max_length=512)\n",
    "sample_dataset = Dataset.from_dict(sample_encoding)\n",
    "sample_dataset = sample_dataset.with_format(\"torch\")\n",
    "\n",
    "# predict\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size=1)\n",
    "tokens = []\n",
    "labels = []\n",
    "for batch in sample_dataloader:\n",
    "  # predict\n",
    "  with torch.no_grad():\n",
    "    output = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "  predicted_label_id = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "  # create output\n",
    "  tokens.append(xlmr_tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0]))\n",
    "  labels.append([index2tag[i] for i in predicted_label_id[0]])\n",
    "\n",
    "# show the first result\n",
    "pd.DataFrame([tokens[0], labels[0]], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81763b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749914e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ac0a06",
   "metadata": {},
   "source": [
    "# индийский туториал:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5468b5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b57b96285f432db9fb1d1c3e55a068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f876c0ebaf437e8b17bf566c67bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aac92069e3475a860cc34360634596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313e3303b9f04343a8e26217f615e366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_adjust_labels at 0x7ff483fd9c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#Get the values for input_ids, token_type_ids, attention_mask\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True)\n",
    "  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "  #so the new keys [input_ids, labels (after adjustment)]\n",
    "  #can be added to the datasets dict for each train test validation split\n",
    "  total_adjusted_labels = []\n",
    "  print(len(tokenized_samples[\"input_ids\"]))\n",
    "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "    prev_wid = -1\n",
    "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "    i = -1\n",
    "    adjusted_label_ids = []\n",
    "   \n",
    "    for wid in word_ids_list:\n",
    "      if(wid is None):\n",
    "        adjusted_label_ids.append(-100)\n",
    "      elif(wid!=prev_wid):\n",
    "        i = i + 1\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        prev_wid = wid\n",
    "      else:\n",
    "        label_name = label_names[existing_label_ids[i]]\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "    total_adjusted_labels.append(adjusted_label_ids)\n",
    "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "  return tokenized_samples\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "700edf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "73e18355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m326.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from seqeval) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from seqeval) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=d2740a2a6d47645ffb1e3daf342f0b78a5f6c876d6a8c0f5e171c5f8cd4e8d18\n",
      "  Stored in directory: /Users/pabakst/Library/Caches/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "61fc84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8f52b4ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [276]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flattened_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall_precision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m flattened_results\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "flattened_results = {\"overall_precision\": results[\"overall_precision\"],\"overall_recall\": results[\"overall_recall\"],\"overall_f1\": results[\"overall_f1\"],\"overall_accuracy\": results[\"overall_accuracy\"],}\n",
    "\n",
    "for k in results.keys():\n",
    "    if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "68b32699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m707.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m544.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m584.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-macosx_10_9_x86_64.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests<3,>=2.0.0->wandb)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=4178d2100553015a271602727236fda3ac1767c752372ebed5c138c6663098fb\n",
      "  Stored in directory: /Users/pabakst/Library/Caches/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, urllib3, setproctitle, docker-pycreds, sentry-sdk, GitPython, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: GitPython\n",
      "    Found existing installation: GitPython 3.1.29\n",
      "    Uninstalling GitPython-3.1.29:\n",
      "      Successfully uninstalled GitPython-3.1.29\n",
      "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.25.1 setproctitle-1.3.2 urllib3-1.26.16 wandb-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "efaa138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "os.environ[\"WANDB_API_KEY\"]=\"API KEY GOES HERE\"\n",
    "os.environ[\"WANDB_ENTITY\"]=\"Suchandra\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"finetune_bert_ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "78b3dab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f51d149e87d4a238e524b9c4d12cf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: spans, ner_tags, langs, tokens. If spans, ner_tags, langs, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: user is not logged in (<Response [401]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /Users/pabakst/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py 593 setup\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "The API key you provided is either invalid or missing.  If the `WANDB_API_KEY` environment variable is set, make sure it is correct. Otherwise, to resolve this issue, you may try running the 'wandb login --relogin' command. If you are using a local server, make sure that you're using the correct hostname. If you're not sure, you can try logging in again using the 'wandb login --relogin --host [hostname]' command.(Error 401: Unauthorized)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [281]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tune_bert_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:1354\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m   1352\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;66;03m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mignore_data_skip:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py:347\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    346\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/trainer_callback.py:388\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 388\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py:621\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/transformers/integrations.py:593\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrun_name\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1171\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1170\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m-> 1171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1152\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1150\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Conda/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:768\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: The API key you provided is either invalid or missing.  If the `WANDB_API_KEY` environment variable is set, make sure it is correct. Otherwise, to resolve this issue, you may try running the 'wandb login --relogin' command. If you are using a local server, make sure that you're using the correct hostname. If you're not sure, you can try logging in again using the 'wandb login --relogin --host [hostname]' command.(Error 401: Unauthorized)"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(label_names_our))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_bert_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 1000,\n",
    "    report_to=\"wandb\",\n",
    "    run_name = \"ep_10_tokenized_11\",\n",
    "    save_strategy='no'\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99a0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
