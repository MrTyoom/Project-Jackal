{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"commands_init.txt\", \"r\") as fp:\n",
    "    commands = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\n",
    "    \"влево\",\n",
    "    \"левее\",\n",
    "    \"налево\",\n",
    "    \"слева\",\n",
    "    \"вправо\",\n",
    "    \"правее\",\n",
    "    \"направо\",\n",
    "    \"справа\",\n",
    "    \"вверх\",\n",
    "    \"выше\",\n",
    "    \"вперед\",\n",
    "    \"наверх\",\n",
    "    \"сверху\",\n",
    "    \"вниз\",\n",
    "    \"ниже\",\n",
    "    \"назад\",\n",
    "    \"снизу\",\n",
    "    \"влево вверх\",\n",
    "    \"по диагонали влево вверх\",\n",
    "    \"влево вниз\",\n",
    "    \"по диагонали влево вниз\",\n",
    "    \"вправо вверх\",\n",
    "    \"по диагонали вправо вверх\",\n",
    "    \"вправо вниз\",\n",
    "    \"по диагонали вправо вниз\",\n",
    "]\n",
    "\n",
    "ship_directions = [\n",
    "    \"влево\",\n",
    "    \"левее\",\n",
    "    \"налево\",\n",
    "    \"слева\",\n",
    "    \"вправо\",\n",
    "    \"правее\",\n",
    "    \"направо\",\n",
    "    \"справа\",\n",
    "    \"вверх\",\n",
    "    \"выше\",\n",
    "    \"вперед\",\n",
    "    \"наверх\",\n",
    "    \"сверху\",\n",
    "    \"вниз\",\n",
    "    \"ниже\",\n",
    "    \"назад\",\n",
    "    \"снизу\",\n",
    "]\n",
    "\n",
    "tiles = [\n",
    "    \"поляна\",\n",
    "    \"пустышка\",\n",
    "    \"стрелка\",\n",
    "    \"указатель\",\n",
    "    \"конь\",\n",
    "    \"лошадь\",\n",
    "    \"бочка\",\n",
    "    \"лабиринт\",\n",
    "    \"джунгли\",\n",
    "    \"пустыня\",\n",
    "    \"болото\",\n",
    "    \"горы\",\n",
    "    \"лед\",\n",
    "    \"капкан\",\n",
    "    \"ловушка\",\n",
    "    \"крокодил\",\n",
    "    \"людоед\",\n",
    "    \"крепость\",\n",
    "    \"сундук\",\n",
    "    \"деньги\",\n",
    "    \"сокровища\",\n",
    "    \"воздушный шар\",\n",
    "    \"шар\",\n",
    "    \"самолет\",\n",
    "    \"пушка\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {\n",
    "    \"move_ship_by_direction\": \"ship_direction\",\n",
    "    \"move_pirate_by_direction\": \"direction\",\n",
    "    \"move_pirate_by_tile\": \"tile\",\n",
    "    \"pirate_swim\": \"ship_direction\",\n",
    "    \"choose_your_fighter\":\"fighter\",\n",
    "    \"move_on_horse\":\"horse_direction\"\n",
    "}\n",
    "entity_examples = {\n",
    "    \"ship_direction\": ship_directions,\n",
    "    \"direction\": directions,\n",
    "    \"tile\": tiles,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbfSzvL8Wdcm"
   },
   "source": [
    "## Back Translation && Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-0WdVNEZ56X"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "MODEL_NAME = \"cointegrated/rut5-base-paraphraser\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def paraphrase(text, beams=5, grams=4):\n",
    "    x = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
    "    out = model.generate(\n",
    "        **x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R--wrLTDo50z"
   },
   "outputs": [],
   "source": [
    "def paraphrase_all(commands: list):\n",
    "    paraphrased = []\n",
    "    for i, command in enumerate(commands):\n",
    "        paraphrased.append(paraphrase(command))  # appends 1 command\n",
    "    return paraphrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H3QhyiipJS6",
    "outputId": "086ce4da-e46f-40cc-8817-6eb42a3ca9e8"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# load models\n",
    "language_code = \"sl\"\n",
    "target_model_name = f\"Helsinki-NLP/opus-mt-ru-{language_code}\"\n",
    "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "back_model_name = f\"Helsinki-NLP/opus-mt-{language_code}-ru\"\n",
    "back_tokenizer = MarianTokenizer.from_pretrained(back_model_name)\n",
    "back_model = MarianMTModel.from_pretrained(back_model_name)\n",
    "\n",
    "\n",
    "def back_translation(batch_texts: list):\n",
    "    # translate\n",
    "    translated_commands = perform_translation(\n",
    "        batch_texts, target_model, target_tokenizer, \"sl\"\n",
    "    )\n",
    "    back_translated_commands = perform_translation(\n",
    "        translated_commands, back_model, back_tokenizer, \"ru\"\n",
    "    )\n",
    "    return back_translated_commands\n",
    "\n",
    "\n",
    "def perform_translation(batch_texts: list, model, tokenizer, target_language: str):\n",
    "    translated = model.generate(\n",
    "        **tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n",
    "    )\n",
    "    translated_texts = [\n",
    "        tokenizer.decode(t, skip_special_tokens=True) for t in translated\n",
    "    ]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH8v9PGSpRsp"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def merge_2(original_commands: list, augmented_commands: list):\n",
    "    new_items = set(augmented_commands) - set(original_commands)\n",
    "    merge_result = original_commands + list(new_items)\n",
    "    return merge_result\n",
    "\n",
    "\n",
    "def merge(lists_to_merge: list):\n",
    "    res = []\n",
    "    for i, l in enumerate(lists_to_merge):\n",
    "        l = [\n",
    "            (re.sub(\"[^А-яа-я]+\", \" \", item[0]).lower(), item[1].lower()) for item in l\n",
    "        ]\n",
    "        res = merge_2(res, l)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWVjg5BuRlXU"
   },
   "outputs": [],
   "source": [
    "def first_augmentation(commands: list):\n",
    "    \"\"\"Composition of back translation and paraphrasing\"\"\"\n",
    "    origs = commands * 5\n",
    "    commands_sl = back_translation(commands)\n",
    "    commands_paraphrased = paraphrase_all(commands)\n",
    "    paraphrase_slovenian = paraphrase_all(commands_sl)\n",
    "    to_slovenian_paraphrased = back_translation(commands_paraphrased)\n",
    "    assert (\n",
    "        len(commands_paraphrased)\n",
    "        + len(commands_sl)\n",
    "        + len(paraphrase_slovenian)\n",
    "        + len(to_slovenian_paraphrased)\n",
    "        == len(commands) * 4\n",
    "    ), \"commands amounts don't match\"\n",
    "    all_commands = merge(\n",
    "        [\n",
    "            [(commands[i], commands[i]) for i in range(len(commands))],\n",
    "            [(commands_paraphrased[i], commands[i]) for i in range(len(commands))],\n",
    "            [(commands_sl[i], commands[i]) for i in range(len(commands))],\n",
    "            [(paraphrase_slovenian[i], commands[i]) for i in range(len(commands))],\n",
    "            [(to_slovenian_paraphrased[i], commands[i]) for i in range(len(commands))],\n",
    "        ]\n",
    "    )\n",
    "    return all_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for command_with_entity in entities.keys():\n",
    "    command_variants = commands[command_with_entity]\n",
    "    entity = entities[command_with_entity]\n",
    "    entity_variants = entity_examples[entity]\n",
    "    new_variants = []\n",
    "    for variant in command_variants:\n",
    "        for entity_variant in entity_variants:\n",
    "            new_variants.append(variant.replace(entity, entity_variant))\n",
    "    commands[command_with_entity] = new_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstly_augmented_commands = {}\n",
    "\n",
    "for command in commands.keys():\n",
    "    firstly_augmented_commands[command] = first_augmentation(commands[command])\n",
    "    print(f\"{command} -- done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted = 0\n",
    "for intent in firstly_augmented_commands.keys():\n",
    "    clean_commands = []\n",
    "    for command in firstly_augmented_commands[intent]:\n",
    "        first_word = (command[0].split(\" \"))[0]\n",
    "        if command[0].count(first_word) > 4:\n",
    "            deleted += 1\n",
    "        else:\n",
    "            clean_commands.append(command)\n",
    "    firstly_augmented_commands[intent] = clean_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"first_augm_big.txt\", \"w\") as fp:\n",
    "    json.dump(firstly_augmented_commands, fp)  # encode dict into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"first_augm_big.txt\", \"r\") as fp:\n",
    "    commands = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = firstly_augmented_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT3HmYqxuBf0"
   },
   "source": [
    "## Defne EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pm5FL-Nt05f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "stop_words = [\n",
    "    \"я\",\n",
    "    \"мы\",\n",
    "    \"наш\",\n",
    "    \"мой\",\n",
    "    \"сейчас\",\n",
    "    \"тогда\",\n",
    "    \"он\",\n",
    "    \"его\",\n",
    "    \"она\",\n",
    "    \"ее\",\n",
    "    \"для\",\n",
    "    \"на\",\n",
    "    \"в\",\n",
    "    \"по\",\n",
    "    \"это\",\n",
    "    \"этот\",\n",
    "    \"с\",\n",
    "    \"от\",\n",
    "    \"при\",\n",
    "    \"к\",\n",
    "    \"быть\",\n",
    "    \"право\",\n",
    "    \"вправо\",\n",
    "    \"направо\",\n",
    "    \"правый\",\n",
    "    \"правее\",\n",
    "    \"лево\",\n",
    "    \"влево\",\n",
    "    \"налево\",\n",
    "    \"левый\",\n",
    "    \"левее\",\n",
    "    \"слева\",\n",
    "    \"второй\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BymCpsNuZ6r"
   },
   "source": [
    "#### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPg4Jp-guU8J"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "def format_command(command):\n",
    "    result = command\n",
    "    result = re.sub(\" +\", \" \", result)  # delete extra spaces\n",
    "    if result != \"\":\n",
    "        if result[0] == \" \":\n",
    "            result = result[1:]\n",
    "        if result[-1] == \" \":\n",
    "            result = result[:-1]\n",
    "    return result\n",
    "\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def to_normal_form(command: str):\n",
    "    words = command.split(\" \")\n",
    "    new_command = \"\"\n",
    "    for word in words:\n",
    "        new_command += morph.parse(word)[0].normal_form + \" \"\n",
    "    return new_command[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_tguyJWu7ZH"
   },
   "source": [
    "#### Swap words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preposition_index(words, word_idx):\n",
    "    if word_idx > 0:\n",
    "        if (\n",
    "            morph.parse(words[word_idx - 1])[0].tag.POS == \"PREP\"\n",
    "            or morph.parse(words[word_idx - 1])[0].tag.POS == \"PRCL\"\n",
    "        ):\n",
    "            return word_idx - 1\n",
    "    if word_idx != len(words) - 1:\n",
    "        if (\n",
    "            morph.parse(words[word_idx])[0].tag.POS == \"PREP\"\n",
    "            or morph.parse(words[word_idx])[0].tag.POS == \"PRCL\"\n",
    "        ):\n",
    "            return word_idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGP-hQquwr6"
   },
   "outputs": [],
   "source": [
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    prep_1 = -1\n",
    "    prep_2 = -1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1 or (prep_1 != -1 and prep_1 == prep_2):\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        counter += 1\n",
    "        prep_1 = preposition_index(new_words, random_idx_1)\n",
    "        prep_2 = preposition_index(new_words, random_idx_2)\n",
    "        if counter > 5:\n",
    "            return new_words\n",
    "\n",
    "    if prep_1 != -1:\n",
    "        random_idx_1 = prep_1\n",
    "        new_words[random_idx_1] += \" \" + new_words[random_idx_1 + 1]\n",
    "    if prep_2 != -1:\n",
    "        random_idx_2 = prep_2\n",
    "        new_words[random_idx_2] += \" \" + new_words[random_idx_2 + 1]\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = (\n",
    "        new_words[random_idx_2],\n",
    "        new_words[random_idx_1],\n",
    "    )\n",
    "\n",
    "    # order indices\n",
    "    if random_idx_1 > random_idx_2:\n",
    "        random_idx_1, random_idx_2 = (random_idx_2, random_idx_1)\n",
    "        prep_1, prep_2 = (prep_2, prep_1)\n",
    "\n",
    "    # fix preps and save next words for insertion\n",
    "    if prep_1 != -1:\n",
    "        prep_1_next_word = new_words.pop(random_idx_1 + 1)\n",
    "        random_idx_2 -= 1  # deleted element before it\n",
    "        new_words[random_idx_2] = new_words[random_idx_2].replace(\n",
    "            f\" {prep_1_next_word}\", \"\"\n",
    "        )\n",
    "    if prep_2 != -1:\n",
    "        prep_2_next_word = new_words.pop(random_idx_2 + 1)\n",
    "        new_words[random_idx_1] = new_words[random_idx_1].replace(\n",
    "            f\" {prep_2_next_word}\", \"\"\n",
    "        )\n",
    "\n",
    "    # insert next words\n",
    "    if prep_1 != -1:\n",
    "        new_words.insert(random_idx_2 + 1, prep_1_next_word)\n",
    "    if prep_2 != -1:\n",
    "        new_words.insert(random_idx_1 + 1, prep_2_next_word)\n",
    "\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdw7xbI0AjkP"
   },
   "source": [
    "#### Replace with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnZNS0Lgu-ty"
   },
   "outputs": [],
   "source": [
    "def replace_with_synonym(words: list, n: int):\n",
    "    \"\"\"\n",
    "    Replaces n random words with a synonym\n",
    "    \"\"\"\n",
    "    new_words = words.copy()\n",
    "    words_to_replace = list()\n",
    "    word_position = dict()\n",
    "    for i, word in enumerate(words):\n",
    "        word = to_normal_form(word)\n",
    "        if word not in stop_words:\n",
    "            if is_in_vocab(word):\n",
    "                words_to_replace.append(word)\n",
    "        word_position[word] = i\n",
    "    random.shuffle(words_to_replace)\n",
    "\n",
    "    replaced = 0\n",
    "    for replace_word in words_to_replace:\n",
    "        synonyms = get_synonyms(replace_word)\n",
    "        if len(synonyms) == 0:\n",
    "            continue\n",
    "        # pick random synonym\n",
    "        new_word = format_command(random.choice(synonyms))\n",
    "        new_words[word_position[replace_word]] = new_word\n",
    "        replaced += 1\n",
    "        if replaced == n:\n",
    "            break\n",
    "\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGoN-zQqW3M4"
   },
   "source": [
    "#### Insert words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMELI4kT__kE"
   },
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = to_normal_form(new_words[random.randint(0, len(new_words) - 1)])\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "        if random_word in stop_words or not is_in_vocab(random_word):\n",
    "            continue\n",
    "        synonyms = get_synonyms(random_word)\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words) - 1)\n",
    "    new_words.insert(random_idx, random_synonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgW2z8iMW6U-"
   },
   "source": [
    "#### Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGimjSqcAAb7"
   },
   "outputs": [],
   "source": [
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    sentence = format_command(sentence)\n",
    "    words = sentence.split(\" \")\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    num_words = len(words)\n",
    "\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "\n",
    "    # synonyms\n",
    "    if alpha_sr > 0:\n",
    "        n_sr = max(1, int(alpha_sr * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = replace_with_synonym(words, n_sr)\n",
    "            augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    # insertions\n",
    "    if alpha_ri > 0:\n",
    "        n_ri = max(1, int(alpha_ri * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_insertion(words, n_ri)\n",
    "            augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    # swaps\n",
    "    if alpha_rs > 0:\n",
    "        n_rs = max(1, int(alpha_rs * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_swap(words, n_rs)\n",
    "            augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    augmented_sentences = [format_command(sentence) for sentence in augmented_sentences]\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    # append the original sentence\n",
    "    augmented_sentences.append(sentence)\n",
    "\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePfsNNuLvhcs"
   },
   "source": [
    "### For synonyms (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EWXxCCQ1dzP"
   },
   "outputs": [],
   "source": [
    "tags_conversion = {\n",
    "    \"A\": \"ADJ\",\n",
    "    \"ADV\": \"ADV\",\n",
    "    \"ADVPRO\": \"ADV\",\n",
    "    \"ANUM\": \"ADJ\",\n",
    "    \"APRO\": \"DET\",\n",
    "    \"COM\": \"ADJ\",\n",
    "    \"CONJ\": \"SCONJ\",\n",
    "    \"INTJ\": \"INTJ\",\n",
    "    \"NONLEX\": \"X\",\n",
    "    \"NUM\": \"NUM\",\n",
    "    \"PART\": \"PART\",\n",
    "    \"PR\": \"ADP\",\n",
    "    \"S\": \"NOUN\",\n",
    "    \"SPRO\": \"PRON\",\n",
    "    \"UNKN\": \"X\",\n",
    "    \"V\": \"VERB\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_J1IpngjASE"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "vectors = gensim.downloader.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SVMjldjyk1A"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "def tag_word(word: str):\n",
    "    processed = m.analyze(word)[0]\n",
    "    lemma = processed[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "    pos = processed[\"analysis\"][0][\"gr\"].split(\",\")[0]\n",
    "    pos = pos.split(\"=\")[0].strip()\n",
    "    tagged = lemma + \"_\" + tags_conversion[pos]\n",
    "    return tagged\n",
    "\n",
    "\n",
    "def is_in_vocab(word: str):\n",
    "    try:\n",
    "        word = tag_word(word)\n",
    "        syns = vectors.most_similar(positive=word)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_synonyms(word: str):\n",
    "    synonyms_vec = vectors.most_similar(positive=tag_word(word))\n",
    "    synonyms = [item[0].split(\"_\")[0] for item in synonyms_vec]\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03lndRu-uF3o"
   },
   "source": [
    "## Augment with EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwh_-EssbbQI"
   },
   "outputs": [],
   "source": [
    "total_commands_count = 0\n",
    "for intent in commands.keys():\n",
    "    total_commands_count += len(commands[intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPbLA4AnuFRR"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "progress_percentage = 0\n",
    "augmented_commands = {}\n",
    "\n",
    "for intent in commands.keys():\n",
    "    all_commands = commands[intent]\n",
    "    augmented_all = []\n",
    "    for command in all_commands:\n",
    "        augmented_list = eda(command[0])\n",
    "        augmented = [\n",
    "            (format_command(variant), command[1]) for variant in augmented_list\n",
    "        ]\n",
    "        augmented_all = merge([augmented_all, augmented])\n",
    "\n",
    "        count += 1\n",
    "        if round((count / total_commands_count) * 100) > progress_percentage:\n",
    "            progress_percentage = round((count / total_commands_count) * 100)\n",
    "            if progress_percentage % 10 == 0:\n",
    "                print(f\"{progress_percentage}% done...\")\n",
    "    augmented_commands[intent] = augmented_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Iy78KMUZMgM"
   },
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBxaiQXkod3Z"
   },
   "outputs": [],
   "source": [
    "intent_column = []\n",
    "command_column = []\n",
    "orig_column = []\n",
    "entity_column = []\n",
    "\n",
    "entities = {\n",
    "    \"move_ship_by_direction\": \"ship_direction\",\n",
    "    \"move_pirate_by_direction\": \"direction\",\n",
    "    \"move_pirate_by_tile\": \"tile\",\n",
    "    \"pirate_swim\": \"ship_direction\",\n",
    "    \"choose_your_fighter\": \"fighter\",\n",
    "    \"move_on_horse\": \"horse_direction\",\n",
    "}\n",
    "\n",
    "for intent in augmented_commands.keys():\n",
    "    for command in augmented_commands[intent]:\n",
    "        command_column.append(command[0])\n",
    "        orig_column.append(command[1])\n",
    "        intent_column.append(intent)\n",
    "        if intent in entities.keys():\n",
    "            entity_column.append(entities[intent])\n",
    "        else:\n",
    "            entity_column.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xbif2VFaPbz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"command\": command_column,\n",
    "        \"orig\": orig_column,\n",
    "        \"intent\": intent_column,\n",
    "        \"entity\": entity_column,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('version-5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import df\n",
    "\n",
    "df = pd.read_csv(\"commands-entity-version-2-with-origs.csv\")\n",
    "\n",
    "# удаляем оригинальные варианты\n",
    "eval_df = df[df[\"command\"] != df[\"orig\"]]\n",
    "\n",
    "test_sentences = list(eval_df[\"command\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "mname = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(mname)\n",
    "gpt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habr version\n",
    "\n",
    "\n",
    "def get_gpt2_ppl(test_sentences, aggregate=True, sep=\"\\n\"):\n",
    "    \"\"\"Calculate average perplexity per token and number of tokens in each text.\"\"\"\n",
    "    lls = []\n",
    "    weights = []\n",
    "    for text in tqdm(test_sentences):\n",
    "        encodings = gpt_tokenizer(f\"{sep}{text}{sep}\", return_tensors=\"pt\")\n",
    "        input_ids = encodings.input_ids.to(gpt_model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        w = max(0, len(input_ids[0]) - 1)\n",
    "        if w > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = gpt_model(input_ids, labels=target_ids)\n",
    "                log_likelihood = outputs[0]\n",
    "                ll = log_likelihood.item()\n",
    "        else:\n",
    "            ll = 0\n",
    "        lls.append(ll)\n",
    "        weights.append(w)\n",
    "\n",
    "    likelihoods, weights = np.array(lls), np.array(weights)\n",
    "    if aggregate:\n",
    "        return sum(likelihoods * weights) / sum(weights)\n",
    "    return likelihoods, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface version\n",
    "\n",
    "\n",
    "def ppl(test_sentences, sep=\"\\n\"):\n",
    "    \"\"\"Calculate average perplexity per token and number of tokens in each text.\"\"\"\n",
    "    lls = []\n",
    "    for text in tqdm(test_sentences):\n",
    "        encodings = gpt_tokenizer(f\"{sep}{text}{sep}\", return_tensors=\"pt\")\n",
    "        input_ids = encodings.input_ids.to(gpt_model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt_model(input_ids, labels=target_ids)\n",
    "            ll = outputs.loss\n",
    "        lls.append(ll)\n",
    "    return torch.exp(torch.stack(lls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpt2_ppl(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение смысла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_metrics(predictions, references):\n",
    "    bleu_results = bleu.compute(predictions=predictions, references=references)\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=predictions, references=references, tokenizer=lambda x: x.split()\n",
    "    )\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=predictions, references=references, lang=\"ru\"\n",
    "    )\n",
    "    results = {\n",
    "        \"bleu\": bleu_results,\n",
    "        \"rouge\": rouge_results,\n",
    "        \"bertscore\": bertscore_results,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = eval_df[\"orig\"]\n",
    "predictions = eval_df[\"command\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_similarity_metrics(predictions, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precisions (list of floats): geometric mean of n-gram precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"bleu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"rouge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_results = results[\"bertscore\"]\n",
    "print(f\"mean precision: {np.mean(bertscore_results['precision'])}\")\n",
    "print(f\"mean recall: {np.mean(bertscore_results['recall'])}\")\n",
    "print(f\"mean f1: {np.mean(bertscore_results['f1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLRbDat4o7vj"
   },
   "source": [
    "## Currently Unused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYZdVNw-pBgA"
   },
   "source": [
    "### Back Translation: other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdaQRnr5o6ME"
   },
   "outputs": [],
   "source": [
    "def back_traslation(batch_texts: list, language_code: str):\n",
    "    \n",
    "    # load models\n",
    "    target_model_name = f'Helsinki-NLP/opus-mt-ru-{language_code}'\n",
    "    target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "    target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "    back_model_name = f'Helsinki-NLP/opus-mt-{language_code}-ru'\n",
    "    back_tokenizer = MarianTokenizer.from_pretrained(back_model_name)\n",
    "    back_model = MarianMTModel.from_pretrained(back_model_name)\n",
    "    \n",
    "    # translate\n",
    "    translated_commands = perform_translation(original_commands, target_model, target_tokenizer, language_code)\n",
    "    back_translated_commands = perform_translation(translated_commands, back_model, back_tokenizer, 'ru')\n",
    "    return back_translated_commands\n",
    "    \n",
    "\n",
    "\n",
    "def perform_translation(batch_texts: list, model, tokenizer, target_language: str):\n",
    "    translated = model.generate(**tokenizer(batch_texts, return_tensors=\"pt\", padding=True))\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtEIUsl0pKM7"
   },
   "outputs": [],
   "source": [
    "original_commands = ['подвинь корабль вправо',\n",
    " 'плыви вправо',\n",
    " 'плыву направо',\n",
    " 'двигаюсь кораблем вправо',\n",
    " 'право руля',\n",
    " 'уплыви вправо',\n",
    " 'отчаливаю направо',\n",
    " 'судно направо',\n",
    " 'мы поплывем правее', \n",
    " 'на лодке вправо',\n",
    " 'врубай мотор идем на восток',\n",
    " 'кораблем направо',\n",
    " 'я иду кораблем направо',\n",
    " 'корабль на клетку вправо',\n",
    " 'шаг правее кораблем']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XKHDb9RpMwf"
   },
   "outputs": [],
   "source": [
    "# английский\n",
    "back_traslation(original_commands, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T35rcEVFpPzp"
   },
   "outputs": [],
   "source": [
    "# французский\n",
    "back_traslation(original_commands, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adUWJh-0pSEn"
   },
   "outputs": [],
   "source": [
    "# украинский\n",
    "back_traslation(original_commands, 'uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJbkzEtIpUGv"
   },
   "outputs": [],
   "source": [
    "# словенский (класс!)\n",
    "back_traslation(original_commands, 'sl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf3cTarMpWVo"
   },
   "outputs": [],
   "source": [
    "# латышский\n",
    "back_traslation(original_commands, 'lv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc1vHD9GpYN1"
   },
   "outputs": [],
   "source": [
    "augmented_by_sl = merge([original_commands, back_traslation(original_commands, 'sl')])\n",
    "augmented_by_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGD1DlWJpg1Q"
   },
   "source": [
    "### Тезаурус RuWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZWa-Q2qpp2q"
   },
   "source": [
    "- [Тезаурус для русского языка](https://github.com/avidale/python-ruwordnet)\n",
    "- Основные сущности:\n",
    "    - Sense - одно слово/словосочетание с конкретным значением\n",
    "    - Synset - множество Sense'ов с одинаковыми значениями и  частью речи\n",
    "- Поддерживаются разлтичные отношения между синсетами:\n",
    "    <div>\n",
    "<img src=\"attachment:image.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd42qxe5pel9"
   },
   "outputs": [],
   "source": [
    "from ruwordnet import RuWordNet\n",
    "\n",
    "wn = RuWordNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxW3otXjp2UP"
   },
   "source": [
    "В EDA использовались такие методы вместо генерации синонимов word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAlujGgApwY9"
   },
   "outputs": [],
   "source": [
    "def replace_with_related(words: list, n: int, relation_type: str):\n",
    "    \"\"\"\n",
    "    Replaces n random words with a synonym or hyponym \n",
    "    (based on relation_type param).\n",
    "    \"\"\"\n",
    "    new_words = words.copy()\n",
    "    words_to_replace = list()\n",
    "    word_position = dict()\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in stop_words:\n",
    "            words_to_replace.append(word)\n",
    "        word_position[word] = i\n",
    "    random.shuffle(words_to_replace)\n",
    "    \n",
    "    replaced = 0\n",
    "    for replace_word in words_to_replace:\n",
    "        related = get_related(replace_word, relation_type)\n",
    "        if len(related) == 0:\n",
    "            continue\n",
    "        # pick random synonym/hyponym\n",
    "        new_word = format_command(random.choice(related))\n",
    "        new_words[word_position[replace_word]] = new_word\n",
    "        replaced += 1\n",
    "        if replaced == n:\n",
    "            break\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def get_related(word: str, relation_type:str):\n",
    "    related = []\n",
    "    if (relation_type == 'synonym'):\n",
    "        for sense in wn.get_senses(word):\n",
    "            related.append(sense.synset.title)\n",
    "    else: # hyponyms\n",
    "        for sense in wn.get_senses(word):\n",
    "            for item in sense.synset.hyponyms:\n",
    "                related.append(item.title)\n",
    "    return related"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4v3EZd0OVAyF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
