{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWCvKCZ89t5j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWCvKCZ89t5j",
    "outputId": "fabebe91-4ddd-4cfa-d8fa-d9d6d93b665a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e58f7",
   "metadata": {},
   "source": [
    "#### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a57cd",
   "metadata": {
    "id": "626a57cd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('commands-entity-version-3-with-origs-cleaned.csv')\n",
    "backwards = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69025f",
   "metadata": {
    "id": "ff69025f"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657294a",
   "metadata": {},
   "source": [
    "#### Осталвяем только с хорошей перплексией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kM5xGKjFzBfv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kM5xGKjFzBfv",
    "outputId": "6909a33e-2665-4ee9-cbe4-5eefc3b43d81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "mname = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(mname)\n",
    "gpt_model.cuda()\n",
    "\n",
    "# Habr version\n",
    "\n",
    "\n",
    "def get_gpt2_ppl(test_sentences, aggregate=True, sep=\"\\n\"):\n",
    "    \"\"\"Calculate average perplexity per token and number of tokens in each text.\"\"\"\n",
    "    lls = []\n",
    "    weights = []\n",
    "    for text in test_sentences:\n",
    "        encodings = gpt_tokenizer(f\"{sep}{text}{sep}\", return_tensors=\"pt\")\n",
    "        input_ids = encodings.input_ids.to(gpt_model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        w = max(0, len(input_ids[0]) - 1)\n",
    "        if w > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = gpt_model(input_ids, labels=target_ids)\n",
    "                log_likelihood = outputs[0]\n",
    "                ll = log_likelihood.item()\n",
    "        else:\n",
    "            ll = 0\n",
    "        lls.append(ll)\n",
    "        weights.append(w)\n",
    "\n",
    "    likelihoods, weights = np.array(lls), np.array(weights)\n",
    "    if aggregate:\n",
    "        return sum(likelihoods * weights) / sum(weights)\n",
    "    return likelihoods, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05a412",
   "metadata": {
    "id": "3f05a412"
   },
   "outputs": [],
   "source": [
    "commands = list(df['command'])\n",
    "origs = list(df['orig'])\n",
    "\n",
    "all_pairs=[]\n",
    "for i in range(len(commands)):\n",
    "    if get_gpt2_ppl([commands[i]]) <= 6.2:\n",
    "        if get_gpt2_ppl([origs[i]]) <= 6.2:\n",
    "            all_pairs.append((commands[i], origs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KBu0PWNIEvJW",
   "metadata": {
    "id": "KBu0PWNIEvJW"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/gdrive/My Drive/all_pairs\", \"w\") as fp:\n",
    "    json.dump(all_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B-N2IsmfFdsm",
   "metadata": {
    "id": "B-N2IsmfFdsm"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/gdrive/My Drive/all_pairs\", \"r\") as fp:\n",
    "    all_pairs = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14f436",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e6a1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "454e6a1a",
    "outputId": "8dfbb231-e8cd-4870-ebef-b36aecdae6f7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rut5-base-paraphraser\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-paraphraser\")\n",
    "\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text, target_text = self.data[idx]\n",
    "        input_tokens = tokenizer.encode_plus(input_text, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                             return_tensors=\"pt\")\n",
    "        target_tokens = tokenizer.encode_plus(target_text, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                              return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_tokens[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_tokens[\"attention_mask\"].squeeze(),\n",
    "            \"decoder_input_ids\": target_tokens[\"input_ids\"].squeeze(),\n",
    "            \"decoder_attention_mask\": target_tokens[\"attention_mask\"].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef542e4",
   "metadata": {
    "id": "4ef542e4"
   },
   "outputs": [],
   "source": [
    "train_data = all_pairs[: int(len(all_pairs) * 0.8)]\n",
    "val_data = all_pairs[int(len(all_pairs) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98a323",
   "metadata": {
    "id": "ba98a323"
   },
   "outputs": [],
   "source": [
    "train_dataset = ParaphraseDataset(train_data)\n",
    "val_dataset = ParaphraseDataset(val_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b85a9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46b85a9e",
    "outputId": "63bd23ff-0563-45f2-b8a8-345ae5e25b09"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MUVItJ_wCwXF",
   "metadata": {
    "id": "MUVItJ_wCwXF"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32bedb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45be87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd45be87",
    "outputId": "38e3dd52-a131-491a-c518-bf05124db612"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                        decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask,\n",
    "                        labels=decoder_input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask,\n",
    "                            labels=decoder_input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Оценка модели и сохранение весов\n",
    "    model_save_name = f'paraphraser-{epoch+5}'\n",
    "    path = f\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fcaee",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OUNFcV6EQDgV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUNFcV6EQDgV",
    "outputId": "c9d0287e-6fe9-4b03-b7f1-85fb1569d061"
   },
   "outputs": [],
   "source": [
    "model_save_name = 'paraphraser-3'\n",
    "path = f\"/content/gdrive/My Drive/{model_save_name}\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb0597",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95bb0597",
    "outputId": "f40f2250-0b58-4d9c-dd1c-081cc11d355b"
   },
   "outputs": [],
   "source": [
    "input_sentence = \"топаю через горы\"\n",
    "input_tokens = tokenizer.encode_plus(input_sentence, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                     return_tensors=\"pt\")\n",
    "input_tokens = {k: v.to(device) for k, v in input_tokens.items()}\n",
    "\n",
    "num_return_sequences = 10\n",
    "paraphrases = []\n",
    "for _ in range(num_return_sequences):\n",
    "    generated_ids = model.generate(input_tokens[\"input_ids\"], attention_mask=input_tokens[\"attention_mask\"],\n",
    "                                   max_length=128, num_return_sequences=1, do_sample=True, top_k=30, top_p=0.95)\n",
    "\n",
    "    generated_ids = generated_ids.cpu()\n",
    "    paraphrase = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    paraphrases.append(paraphrase)\n",
    "\n",
    "print(f\"Input sentence: {input_sentence}\\n\")\n",
    "for paraphrase in paraphrases:\n",
    "    print(paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('commands-entity-version-3-with-origs-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = list(df['command'])\n",
    "predictions = []\n",
    "for ref in references:\n",
    "    input_sentence = ref\n",
    "    input_tokens = tokenizer.encode_plus(input_sentence, padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                 return_tensors=\"pt\")\n",
    "    input_tokens = {k: v.to(device) for k, v in input_tokens.items()}\n",
    "    generated_ids = model.generate(input_tokens[\"input_ids\"], attention_mask=input_tokens[\"attention_mask\"],\n",
    "                               max_length=128, num_return_sequences=1, do_sample=True, top_k=30, top_p=0.95)\n",
    "\n",
    "    generated_ids = generated_ids.cpu()\n",
    "    paraphrase = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    predictions.append(paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/content/paraphrase-pred\", \"w\") as fp:\n",
    "    json.dump(predictions, fp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
